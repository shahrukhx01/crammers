{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rope Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RoPEEmbedding(torch.nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        \"\"\"\n",
    "        Rotary Positional Embedding (RoPE) implementation in PyTorch.\n",
    "        \n",
    "        Args:\n",
    "        - dim: The embedding dimension (must be even).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert dim % 2 == 0, \"Embedding dimension must be even for RoPE.\"\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Applies RoPE to input tensor.\n",
    "\n",
    "        Args:\n",
    "        - x: Tensor of shape (batch_size, seq_len, dim)\n",
    "\n",
    "        Returns:\n",
    "        - Tensor with the same shape, but with RoPE applied.\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        theta = 10000 ** (-torch.arange(0, self.dim, 2, dtype=torch.float32) / self.dim)\n",
    "        position = torch.arange(seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "        freqs = position * theta.unsqueeze(0)  # (seq_len, dim/2)\n",
    "\n",
    "        # Compute sin and cos components\n",
    "        sin = torch.sin(freqs).to(x.device)\n",
    "        cos = torch.cos(freqs).to(x.device)\n",
    "\n",
    "        # Reshape input tensor for rotation\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]  # Even and odd parts\n",
    "\n",
    "        # Apply rotation\n",
    "        rotated_x = torch.cat([x1 * cos - x2 * sin, x1 * sin + x2 * cos], dim=-1)\n",
    "        return rotated_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 8]) torch.Size([2, 5, 8])\n",
      "torch.Size([2, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 5\n",
    "dim = 16  # Must be even\n",
    "\n",
    "rope = RoPEEmbedding(dim)\n",
    "x = torch.randn(batch_size, seq_len, dim)  # Example input\n",
    "output = rope(x)\n",
    "\n",
    "print(output.shape)  # Should be (batch_size, seq_len, dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoummblock11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
